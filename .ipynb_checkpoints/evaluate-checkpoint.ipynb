{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of seq is 300\n",
      "Dimension is 25\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "\n",
    "from MAESeqModule.MAESeq_utils import dataloader,get_dict, seq_data_to_onehot\n",
    "import numpy as np\n",
    "DICT_EXIST = True\n",
    "DICT_PATH_CHAR2INT = 'dict/char2int.npy'\n",
    "DICT_PATH_INT2CHAR = 'dict/int2char.npy'\n",
    "DATA_PATH = 'dataset/scop_fa_represeq_lib_latest.fa'\n",
    "\n",
    "seq_list, max_len = dataloader(\n",
    "    file= DATA_PATH,\n",
    "    len_data=10000, max_len_percintile=75)\n",
    "max_len = 300 # 在这里统一一下\n",
    "\n",
    "if DICT_EXIST:\n",
    "    dict_char2int = np.load(DICT_PATH_CHAR2INT, allow_pickle = True).item()\n",
    "    dict_int2char = np.load(DICT_PATH_INT2CHAR,allow_pickle = True).item()\n",
    "else:\n",
    "    dict_char2int, dict_int2char = get_dict(seq_list)\n",
    "    np.save(DICT_PATH_CHAR2INT, dict_char2int)\n",
    "    np.save(DICT_PATH_INT2CHAR, dict_int2char)\n",
    "\n",
    "onehot_data = seq_data_to_onehot(seq_list,dict_char2int,max_len)\n",
    "dimension = len(dict_char2int)\n",
    "\n",
    "print(\"Length of seq is {}\".format(max_len))\n",
    "print(\"Dimension is {}\".format(dimension))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 300, 25)\n",
      "(1500, 300, 25)\n",
      "(500, 300, 25)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "onehot_train, onehot_val,onehot_test =np.split(onehot_data,[\n",
    "        int(len(onehot_data)*0.8), \n",
    "        int(len(onehot_data)*0.95)])\n",
    "print(onehot_train.shape)\n",
    "print(onehot_val.shape)\n",
    "print(onehot_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117.0\n",
      "117.0\n"
     ]
    }
   ],
   "source": [
    "from MAESeqModule.MAESeq_utils import mask_onehot_matrix\n",
    "\n",
    "onehot_train_mask = mask_onehot_matrix(onehot_train, 0)\n",
    "\n",
    "i = 40\n",
    "print(np.sum(onehot_train[i]))\n",
    "print(np.sum(onehot_train_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-01 17:36:38.831923: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-01 17:36:41.380621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 51342 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:c2:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from MAESeqModule.MAESeq_model import AutoencoderGRU, ReconstructRateVaried,my_loss_entropy\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "autoencoder = keras.models.load_model('/geniusland/home/qufuchuan/trained_model/',custom_objects={\n",
    "    'my_loss_entropy':my_loss_entropy, \n",
    "    'ReconstructRateVaried':ReconstructRateVaried})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start evaluate mask rate = 0.00\n",
      "start evaluate mask rate = 0.05\n",
      "start evaluate mask rate = 0.10\n",
      "start evaluate mask rate = 0.15\n",
      "start evaluate mask rate = 0.20\n",
      "start evaluate mask rate = 0.25\n",
      "start evaluate mask rate = 0.30\n",
      "start evaluate mask rate = 0.35\n",
      "start evaluate mask rate = 0.40\n",
      "start evaluate mask rate = 0.45\n",
      "start evaluate mask rate = 0.50\n",
      "start evaluate mask rate = 0.55\n",
      "start evaluate mask rate = 0.60\n",
      "start evaluate mask rate = 0.65\n",
      "start evaluate mask rate = 0.70\n",
      "start evaluate mask rate = 0.75\n",
      "start evaluate mask rate = 0.80\n",
      "start evaluate mask rate = 0.85\n",
      "start evaluate mask rate = 0.90\n",
      "start evaluate mask rate = 0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mask 0.00    1.000000\n",
       "Mask 0.05    0.941650\n",
       "Mask 0.10    0.889630\n",
       "Mask 0.15    0.842708\n",
       "Mask 0.20    0.795932\n",
       "Mask 0.25    0.750709\n",
       "Mask 0.30    0.704728\n",
       "Mask 0.35    0.659785\n",
       "Mask 0.40    0.613877\n",
       "Mask 0.45    0.568385\n",
       "Mask 0.50    0.521059\n",
       "Mask 0.55    0.477462\n",
       "Mask 0.60    0.431603\n",
       "Mask 0.65    0.385548\n",
       "Mask 0.70    0.341840\n",
       "Mask 0.75    0.295700\n",
       "Mask 0.80    0.251112\n",
       "Mask 0.85    0.205021\n",
       "Mask 0.90    0.160189\n",
       "Mask 0.95    0.115870\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from MAESeqModule.MAESeq_utils import mask_onehot_matrix, evaluate_per_mask_rate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def mask_onehot_matrix(onehot_data, mask_rate = 0.2):\n",
    "    # To input the whole data\n",
    "    len_data = onehot_data.shape[0]\n",
    "    res = onehot_data.copy()\n",
    "\n",
    "    val_len = np.sum(np.sum(onehot_data, axis=2),axis=1)\n",
    "    mask_len = (val_len * mask_rate).astype(np.int32)\n",
    "    right_limit = val_len-mask_len\n",
    "\n",
    "    start_point = np.random.randint(right_limit)\n",
    "    end_point = start_point+mask_len\n",
    "    \n",
    "    # len_seq = onehot_data.shape[1]\n",
    "    # len_mask = int(mask_rate*len_seq)\n",
    "    # for single_matrix in res:\n",
    "    #     mask_choose = np.random.choice(len_seq,len_mask,replace=False)\n",
    "    #     single_matrix[mask_choose,:]=0\n",
    "    for _ in range(len_data):\n",
    "        res[_,start_point[_]:end_point[_],:] = 0.\n",
    "    return res\n",
    "\n",
    "def evaluate_per_mask_rate(onehot_test, autoencoder):\n",
    "    mask_rates = np.linspace(0,1,21)[:-1]\n",
    "    res = pd.Series(dtype=pd.Float64Dtype)\n",
    "    for rate in mask_rates:\n",
    "        print('start evaluate mask rate = %.2f'%rate)\n",
    "        onehot_test_mask = mask_onehot_matrix(onehot_test, rate)\n",
    "        test_res = autoencoder.predict(onehot_test_mask)\n",
    "        reconst_rate = ReconstructRateVaried(onehot_test, test_res)\n",
    "        # reconst_rate = rate\n",
    "        res['Mask '+'%.2f'%rate] = float(reconst_rate)\n",
    "    return res\n",
    "\n",
    "res = evaluate_per_mask_rate(autoencoder=autoencoder, onehot_test = onehot_test)\n",
    "res\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
